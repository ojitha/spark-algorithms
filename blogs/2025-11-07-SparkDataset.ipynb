{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d630-f517-4d8c-bb11-b4d055aec633",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  Spark Dataset APIs\n",
    "date:   2025-11-07\n",
    "categories: [Spark, Scala]\n",
    "mermaid: true\n",
    "maths: true\n",
    "typora-root-url: /Users/ojitha/GitHub/ojitha.github.io\n",
    "typora-copy-images-to: ../../blog/assets/images/${filename}\n",
    "---\n",
    "\n",
    "<style>\n",
    "/* Styles for the two-column layout */\n",
    ".image-text-container {\n",
    "    display: flex; /* Enables flexbox */\n",
    "    flex-wrap: wrap; /* Allows columns to stack on small screens */\n",
    "    gap: 20px; /* Space between the image and text */\n",
    "    align-items: center; /* Vertically centers content in columns */\n",
    "    margin-bottom: 20px; /* Space below this section */\n",
    "}\n",
    "\n",
    ".image-column {\n",
    "    flex: 1; /* Allows this column to grow */\n",
    "    min-width: 250px; /* Minimum width for the image column before stacking */\n",
    "    max-width: 40%; /* Maximum width for the image column to not take up too much space initially */\n",
    "    box-sizing: border-box; /* Include padding/border in element's total width/height */\n",
    "}\n",
    "\n",
    ".text-column {\n",
    "    flex: 2; /* Allows this column to grow more (e.g., twice as much as image-column) */\n",
    "    min-width: 300px; /* Minimum width for the text column before stacking */\n",
    "    box-sizing: border-box;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<div class=\"image-text-container\">\n",
    "    <div class=\"image-column\">\n",
    "        <img src=\"https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-10027-Scala-2-Collections/scala-collections-illustration.svg\" alt=\"Scala Functors\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "    <div class=\"text-column\">\n",
    "<p>TBC</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!--more-->\n",
    "\n",
    "------\n",
    "\n",
    "* TOC\n",
    "{:toc}\n",
    "------\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619071eb-808f-4e4e-b45f-617bc367132f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What are Datasets?\n",
    "\n",
    "Apache Spark Datasets are the foundational type in Spark's Structured APIs, providing a **type-safe**, distributed collection of strongly typed JVM objects. While DataFrames are Datasets of type `Row`, Datasets allow you to define custom domain-specific objects that each row will consist of, combining the benefits of RDDs (type safety, custom objects) with the optimizations of DataFrames (Catalyst optimizer, Tungsten execution).\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "1. **Type Safety**: Compile-time type checking prevents runtime type errors\n",
    "2. **Encoders**: Special serialization mechanism that maps domain-specific types to Spark's internal binary format\n",
    "3. **Catalyst Optimization**: Benefits from Spark SQL's query optimizer\n",
    "4. **JVM Language Feature**: Available only in Scala and Java (not Python or R)\n",
    "5. **Functional API**: Supports functional transformations like `map`, `filter`, `flatMap`\n",
    "\n",
    "**Dataset[T]**: A distributed collection of data elements of type `T`, where `T` is a domain-specific class (case class in Scala, JavaBean in Java) that Spark can encode and optimize.\n",
    "\n",
    "$$\n",
    "\\text{Dataset}[T] = \\{t_1, t_2, \\ldots, t_n\\} \\text{ where } t_i \\in T\n",
    "$$\n",
    "\n",
    "Translation: A Dataset of type T is a collection of n elements, where each element belongs to type T.\n",
    "\n",
    "**Encoder[T]**: A mechanism that converts between JVM objects of type `T` and Spark SQL's internal binary format (InternalRow).\n",
    "\n",
    "$$\n",
    "\\text{Encoder}[T]: T \\leftrightarrow \\text{InternalRow}\n",
    "$$\n",
    "\n",
    "Translation: An Encoder for type T provides bidirectional conversion between objects of type T and Spark's internal row representation.\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "Datasets embody key functional programming concepts:\n",
    "\n",
    "1. **Functor Laws** (for `map`):\n",
    "    - Identity: `ds.map(x => x) = ds`\n",
    "    - Composition: `ds.map(f).map(g) = ds.map(x => g(f(x)))`\n",
    "\n",
    "2. **Monad Laws** (for `flatMap`):\n",
    "    - Left identity: `Dataset(x).flatMap(f) = f(x)`\n",
    "    - Right identity: `ds.flatMap(x => Dataset(x)) = ds`\n",
    "    - Associativity: `ds.flatMap(f).flatMap(g) = ds.flatMap(x => f(x).flatMap(g))`\n",
    "\n",
    "### Dataset Movie Lens\n",
    "\n",
    "Let's examine the MovieLens dataset: [recommended for education and development](https://grouplens.org/datasets/movielens/){:target=\"_blank\"} for simplicity.\n",
    "\n",
    "```mermaid\n",
    "erDiagram\n",
    "    MOVIES ||--o{ RATINGS : \"receives\"\n",
    "    MOVIES ||--o{ TAGS : \"has\"\n",
    "    MOVIES ||--|| LINKS : \"references\"\n",
    "    \n",
    "    MOVIES {\n",
    "        int movieId PK \"Primary Key\"\n",
    "        string title \"Movie title with year\"\n",
    "        string genres \"Pipe-separated genres\"\n",
    "    }\n",
    "    \n",
    "    RATINGS {\n",
    "        int userId FK \"Foreign Key to User\"\n",
    "        int movieId FK \"Foreign Key to Movie\"\n",
    "        float rating \"Rating value (0.5-5.0)\"\n",
    "        long timestamp \"Unix timestamp\"\n",
    "    }\n",
    "    \n",
    "    TAGS {\n",
    "        int userId FK \"Foreign Key to User\"\n",
    "        int movieId FK \"Foreign Key to Movie\"\n",
    "        string tag \"User-generated tag\"\n",
    "        long timestamp \"Unix timestamp\"\n",
    "    }\n",
    "    \n",
    "    LINKS {\n",
    "        int movieId PK \"Primary Key\"\n",
    "        int movieId FK \"Foreign Key to Movie\"\n",
    "        string imdbId \"IMDB identifier\"\n",
    "        string tmdbId \"TMDB identifier\"\n",
    "    }\n",
    "```\n",
    "\n",
    "#### **Entities and Attributes:**\n",
    "\n",
    "1.  **MOVIES** (9,742 movies)\n",
    "    -   `movieId` (Primary Key)\n",
    "    -   `title` (includes release year)\n",
    "    -   `genres` (pipe-separated list)\n",
    "2.  **RATINGS** (100,836 ratings)\n",
    "    -   `userId` (Foreign Key)\n",
    "    -   `movieId` (Foreign Key)\n",
    "    -   `rating` (0.5 to 5.0 stars)\n",
    "    -   `timestamp` (Unix timestamp)\n",
    "3.  **TAGS** (3,683 tags)\n",
    "    -   `userId` (Foreign Key)\n",
    "    -   `movieId` (Foreign Key)\n",
    "    -   `tag` (user-generated metadata)\n",
    "    -   `timestamp` (Unix timestamp)\n",
    "4.  **LINKS** (9,742 links)\n",
    "    -   `movieId` (Primary Key & Foreign Key)\n",
    "    -   `imdbId` (IMDB identifier)\n",
    "    -   `tmdbId` (The Movie Database identifier)\n",
    "\n",
    "#### **Relationships:**\n",
    "\n",
    "-   **MOVIES ↔ RATINGS**: One-to-Many (a movie can have multiple ratings)\n",
    "-   **MOVIES ↔ TAGS**: One-to-Many (a movie can have multiple tags)\n",
    "-   **MOVIES ↔ LINKS**: One-to-One (each movie has one set of external links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b84bbeb-778c-49de-88c8-82a8ec31f328",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Configure Coursier to fetch doc JARs\n",
    "interp.repositories() ++= Seq(\n",
    "coursierapi.MavenRepository.of(\"https://repo1.maven.org/maven2\")\n",
    ")\n",
    "\n",
    "// Enable compiler to use Java classpath (REMOVED the invalid doc.value line)\n",
    "interp.configureCompiler(c => {\n",
    "c.settings.usejavacp.value = true\n",
    "})\n",
    "\n",
    "// Import Spark\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.1` // Or use any other 2.x version here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae42f50-84cb-4be9-ba68-cf5893adb4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.{LogManager, Level}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.core.config.Configurator\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.logging.log4j.{LogManager, Level}\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Set log levels BEFORE creating SparkSession\n",
    "Configurator.setRootLevel(Level.WARN)\n",
    "Configurator.setLevel(\"org.apache.spark\", Level.WARN)\n",
    "Configurator.setLevel(\"org.apache.spark.executor.Executor\", Level.WARN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d061574b-1b6c-4d45-a99c-d3a88690f56c",
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Loading <code>spark-stubs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Getting spark JARs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Creating SparkSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:48:02.941 [scala-interpreter-1] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://5726cb1a6400:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5b4f9fd2\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "//Set logger level to Warn\n",
    "// spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191c7c5-f7ba-4615-9aab-0d9a5bb015d3",
   "metadata": {},
   "source": [
    "Let's define the Case class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38e9196-4be0-4139-b919-339ae269a1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mMovie\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Movie(\n",
    "  movieId: Int,\n",
    "  title: String,\n",
    "  genres: String\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760d8e0-4ac6-4a61-aa85-a9547374f66b",
   "metadata": {},
   "source": [
    "Create a DataSet using the above Case class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b8b93c-1e4d-4ebe-bf10-5a47329cfe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+\n",
      "|movieId|           title|              genres|\n",
      "+-------+----------------+--------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|  Jumanji (1995)|Adventure|Childre...|\n",
      "+-------+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmoviesDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mMovie\u001b[39m] = [movieId: int, title: string ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV and convert to Dataset\n",
    "val moviesDS = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .csv(\"ml-latest-small/movies.csv\")\n",
    "  .as[Movie]\n",
    "\n",
    "// Example queries\n",
    "moviesDS.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee03cc-eb53-443e-bfcc-8cd49df36450",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- Case classes must be serializable\n",
    "- All fields should have Spark-compatible types\n",
    "- The `.as[T]` method performs the conversion from DataFrame to Dataset\n",
    "\n",
    "##### Understanding Encoders\n",
    "\n",
    "Encoders are a critical component of the Dataset API. They provide:\n",
    "\n",
    "1. <span>Efficient Serialisation</span>{:gtxt}: Convert JVM objects to Spark's internal Tungsten binary format\n",
    "2. <span>Schema Generation</span>{:gtxt}: Automatically infer schema from case class structure\n",
    "3. <span>Code Generation</span>{:gtxt}: Enable whole-stage code generation for better performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4da7331-736d-4109-9078-3272f6c763cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Dataset\u001b[39m\n",
       "\u001b[36mintDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInt\u001b[39m] = [value: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "// for primitive types\n",
    "val intDS : Dataset[Int] = Seq(1,2,3).toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1773bb4a-9bea-4d3e-9f93-3b3843fd9979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtupleDS\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = [_1: string, _2: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tupleDS: Dataset[(String, Int)] = Seq((\"a\",1), (\"b\", 2)).toDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f1245-acdd-4fce-9fee-86a6432caad3",
   "metadata": {},
   "source": [
    "Using Case classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a694367-04d2-4b60-878a-769345a9fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mDog\u001b[39m\n",
       "\u001b[36mdogsDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mDog\u001b[39m] = [name: string, age: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Dog(name: String, age: Int)\n",
    "\n",
    "val dogsDS: Dataset[Dog] = Seq(Dog(\"Liela\",3), Dog(\"Tommy\", 5)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3766010a-06c5-4ae6-a31d-a531e626100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Liela|  3|\n",
      "|Tommy|  5|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dogsDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537a3cb-e15e-4b61-8c87-1e95078fe17d",
   "metadata": {},
   "source": [
    "## Dataset Transformations\n",
    "\n",
    "### map Transformation\n",
    "\n",
    "The `map` transformation applies a function to each element in the Dataset, producing a new Dataset with transformed elements. It's a **narrow transformation** (no shuffle required) and maintains a **one-to-one relationship** between input and output elements.\n",
    "\n",
    "```scala\n",
    "def map[U](func: T => U)(implicit encoder: Encoder[U]): Dataset[U]\n",
    "```\n",
    "`f`: function\n",
    "\n",
    "For example, to extract the movie title:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcca7fcf-cae1-4a04-8e2d-a888c6f00f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|value                  |\n",
      "+-----------------------+\n",
      "|Toy Story (1995)       |\n",
      "|Jumanji (1995)         |\n",
      "|Grumpier Old Men (1995)|\n",
      "+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesDS.map(m => m.title).show(3, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b8ccd1-3023-42eb-9a08-422c0cec7217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mextractMovieInfoFun\u001b[39m\n",
       "\u001b[36mres11_1\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = [_1: string, _2: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractMovieInfoFun(movie: Movie): (String, String) = (movie.title, movie.genres)\n",
    "moviesDS.map(extractMovieInfoFun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0be87-8108-411c-8644-74cb73b4d59f",
   "metadata": {},
   "source": [
    "As shown above, you can create a function.\n",
    "\n",
    "Or you can create a anonymous function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e74d90-b018-4205-bc02-46a5e9b84be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mextractMovieInfoAnonymousFun\u001b[39m: \u001b[32mMovie\u001b[39m => (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = ammonite.$sess.cmd12$Helper$$Lambda$6500/771828955@2bdc4462\n",
       "\u001b[36mres12_1\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = [_1: string, _2: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val extractMovieInfoAnonymousFun: Movie => (String, String) = movie => (movie.title, movie.genres)\n",
    "moviesDS.map(extractMovieInfoAnonymousFun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0def72-c627-4c7f-8f36-bb9565b81231",
   "metadata": {},
   "source": [
    "Above can be directly written in the `map` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e19f5ee-9114-455d-8925-67b61e3f3f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = [_1: string, _2: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesDS.map(movie => (movie.title, movie.genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34648bf6-2f59-488a-ad79-0b1fbe03daf2",
   "metadata": {},
   "source": [
    "### flatMap Transformation\n",
    "\n",
    "The `flatMap` transformation applies a function to each element and **flattens** the results. Each input element can produce **zero, one, or multiple output elements**. This is essential for transformations like tokenization, exploding nested structures, or filtering with expansion.\n",
    "\n",
    "```scala\n",
    "def flatMap[U](func: T => TraversableOnce[U])(implicit encoder: Encoder[U]): Dataset[U]\n",
    "```\n",
    "\n",
    "Translation: Given a function that transforms each element of type `T` into a collection of type `U`, flatten all collections into a single Dataset of type `U`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2180cc3d-d4d3-4c6f-85c0-08b5a914a974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mMovieGenres\u001b[39m\n",
       "\u001b[36mgenres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mMovieGenres\u001b[39m] = [id: int, genres: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class MovieGenres (id: Int, genres: String)\n",
    "val genres = moviesDS.map { movie =>\n",
    "    MovieGenres(movie.movieId, movie.genres)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e57b484-cab5-45d8-aee8-195bd4e50b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------+\n",
      "|id |genres                                     |\n",
      "+---+-------------------------------------------+\n",
      "|1  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2  |Adventure|Children|Fantasy                 |\n",
      "|3  |Comedy|Romance                             |\n",
      "+---+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genres.show(3, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01f8785-bc73-4815-b09c-68d69541d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    value|\n",
      "+---------+\n",
      "|Adventure|\n",
      "|Animation|\n",
      "| Children|\n",
      "|   Comedy|\n",
      "|  Fantasy|\n",
      "|Adventure|\n",
      "| Children|\n",
      "|  Fantasy|\n",
      "|   Comedy|\n",
      "|  Romance|\n",
      "|   Comedy|\n",
      "|    Drama|\n",
      "|  Romance|\n",
      "|   Comedy|\n",
      "|   Action|\n",
      "|    Crime|\n",
      "| Thriller|\n",
      "|   Comedy|\n",
      "|  Romance|\n",
      "|Adventure|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgenresDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genresDS = genres.flatMap(m => m.genres.split(\"\\\\|\"))\n",
    "genresDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081f24b-cfe3-400a-ab9b-8d328696b68c",
   "metadata": {},
   "source": [
    "> The `split()` method takes a *regex pattern, and `|` is a special character in regex meaning \"OR\"*{:rtxt}. So `split(\"|\")` doesn't work as expected. *Instead, use `split(\"\\\\|\")` for split*{:gtxt}.\n",
    "{:.yellow}\n",
    "\n",
    "Complex Example: Nested Structure Explosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6273568d-76a9-4e65-b05c-00226b8340ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSentence\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mSentence\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Sentence(id: Int, words: Seq[String], occurrences: Seq[Int])\n",
    "\n",
    "object Sentence {\n",
    "  // Create Sentence from string format \"1: Hello, how are you?\"\n",
    "  def fromMovie(movie: Movie): Sentence = {\n",
    "    val id = movie.movieId\n",
    "    val text = movie.genres\n",
    "    \n",
    "    // Extract words\n",
    "    val words = text.split(\"\\\\|\").toSeq\n",
    "    \n",
    "    // Count occurrences of each word\n",
    "    val wordCounts = words.groupBy(identity).mapValues(_.size).toMap\n",
    "    val occurrences = words.map(word => wordCounts(word))\n",
    "    \n",
    "    Sentence(id, words, occurrences)\n",
    "  }\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff4df117-b608-43c4-ad5f-64acbf253d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msentenceDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mSentence\u001b[39m] = [id: int, words: array<string> ... 1 more field]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sentenceDS = moviesDS.map(Sentence.fromMovie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5639d821-ccae-4234-9316-8c87942df837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+\n",
      "| id|               words|    occurrences|\n",
      "+---+--------------------+---------------+\n",
      "|  1|[Adventure, Anima...|[1, 1, 1, 1, 1]|\n",
      "|  2|[Adventure, Child...|      [1, 1, 1]|\n",
      "|  3|   [Comedy, Romance]|         [1, 1]|\n",
      "+---+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDS.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56d5f081-fdf0-498c-ad64-ed107ef32f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+\n",
      "| _1|       _2| _3|\n",
      "+---+---------+---+\n",
      "|  1|Adventure|  1|\n",
      "|  1|Animation|  1|\n",
      "|  1| Children|  1|\n",
      "|  1|   Comedy|  1|\n",
      "|  1|  Fantasy|  1|\n",
      "+---+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDS.flatMap { sentence =>\n",
    "    sentence.words.zip(sentence.occurrences).map { case (word, numOccured) =>\n",
    "        (sentence.id, word, numOccured)\n",
    "        \n",
    "    }\n",
    "}.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15640cc-d3f6-4b5b-86b2-5d3588c565eb",
   "metadata": {},
   "source": [
    "[^1]: Chambers, B., Zaharia, M., 2018. Spark: The Definitive Guide. Ch. 11: \"Datasets\"\n",
    "\n",
    "[^2]: Holden Karau, Rachel Warren., 2017. High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark. Ch. 3: \"DataFrames, Datasets, and Spark SQL\"\n",
    "\n",
    "[^3]: Chambers, B., Zaharia, M., 2018. Spark: The Definitive Guide. Ch. 13: \"Advanced RDDs\"\n",
    "\n",
    "[^4]: Holden Karau, Rachel Warren., 2017. High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark. Ch. 4: \"Joins (SQL and Core)\"\n",
    "\n",
    "[^5]: Holden Karau, Rachel Warren., 2017. High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark. Ch. 6: \"Working with Key/Value Data\"\n",
    "\n",
    "[^6]: Ryza, Sandy, Laserson, Uri, Owen, Sean, Wills, Josh., 2017. Advanced Analytics with Spark, 2nd Edition. Ch. 2: \"Introduction to Data Analysis with Scala and Spark\"\n",
    "\n",
    "[^7]: [Apache Spark Dataset API Documentation](https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.Dataset) - Scala 2.x API\n",
    "\n",
    "{:gtxt: .message color=\"green\"}\n",
    "{:ytxt: .message color=\"yellow\"}\n",
    "{:rtxt: .message color=\"red\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0664f90-f90f-43c2-ae13-0ce976ab2bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"version 2.12.20\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala.util.Properties.versionString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f6c6ce-35ca-43c2-b34c-fb17fa54ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364aa93-80a7-4f5f-942f-aa2f4039a190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
